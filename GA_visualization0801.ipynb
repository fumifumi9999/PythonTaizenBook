{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa27e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# coding: utf-8\n",
    " \n",
    "\n",
    "#Webサイトのグループ化コード（クリニック領域だけとか、フォームだけとか、正規表現縛りができるとGAでの確認作業が楽になる\n",
    "\n",
    "#配置もちゃんとできるようにする\n",
    "\n",
    " \n",
    "\n",
    "#その他デジマ：問合せフォームと個人情報登録フォームのファネルを出せるようにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fe7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 新データ収集シート対応（ページビュー＝＞ユーザー）\n",
    "df1 = pd.read_csv('D:/Users/fobayashi/Desktop/File/2022April4May20/PreviousPageNextPage_AllUU.csv')\n",
    "PageViewFilter = 2 \n",
    "\n",
    "df2 = pd.read_csv('D:/Users/fobayashi/Desktop/File/2022April4May20/ReferenceMediaPage_AllUU.csv')\n",
    "df3 = pd.read_csv('D:/Users/fobayashi/Desktop/File/2022April4May20/Page_AllUU.csv')\n",
    "df4 = pd.read_csv('D:/Users/fobayashi/Desktop/File/2022April4May20/ReferenceMedia_AllUU.csv')\n",
    "\n",
    "# 前のページ遷移 -> ページ \n",
    "df1_noEnt=df1[df1[\"前のページ遷移\"]!=\"(entrance)\"]\n",
    "df1_exerpt = df1_noEnt[df1_noEnt['ユーザー']>PageViewFilter]\n",
    "\n",
    "# Lamda使って?以下を削除（?formId=があるものは残す）\n",
    "df1_exerpt[\"前のページ遷移\"] = pd.DataFrame(df1_exerpt[\"前のページ遷移\"]).applymap(lambda x: x.split('?')[0] if (x.find('?') and not '?formId=' in x) else x)\n",
    "df1_exerpt[\"ページ\"] = pd.DataFrame(df1_exerpt[\"ページ\"]).applymap(lambda x: x.split('?')[0] if (x.find('?') and not '?formId=' in x) else x)\n",
    "\n",
    "# Lamda使って二つのドメインのみをスコープにする\n",
    "df1_exerpt[\"前のページ遷移\"] = pd.DataFrame(df1_exerpt[\"前のページ遷移\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x) else np.nan)\n",
    "df1_exerpt[\"ページ\"] = pd.DataFrame(df1_exerpt[\"ページ\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x) else np.nan)\n",
    "\n",
    "\n",
    "#Nanになったいらない行を削除\n",
    "df1_exerpt = df1_exerpt.dropna(how='any').reset_index().drop('index', axis=1)\n",
    "\n",
    "#個人所法登録フォーム一元化\n",
    "df1_exerpt[\"前のページ遷移\"] = pd.DataFrame(df1_exerpt[\"前のページ遷移\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/MailForms\" if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' and not x.split('/')[1] == 'form' else x)\n",
    "df1_exerpt[\"前のページ遷移\"] = pd.DataFrame(df1_exerpt[\"前のページ遷移\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/download-thanks\" if x.split('/')[1] == 'download-thanks' else x)\n",
    "df1_exerpt[\"前のページ遷移\"] = pd.DataFrame(df1_exerpt[\"前のページ遷移\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/Forms\" if x.split('/')[1] == 'form' else x)\n",
    "df1_exerpt[\"ページ\"] = pd.DataFrame(df1_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/MailForms\" if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' and not x.split('/')[1] == 'form' else x)\n",
    "df1_exerpt[\"ページ\"] = pd.DataFrame(df1_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/download-thanks\" if x.split('/')[1] == 'download-thanks' else x)\n",
    "df1_exerpt[\"ページ\"] = pd.DataFrame(df1_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/Forms\" if x.split('/')[1] == 'form' else x)\n",
    "\n",
    "# クリニック領域一元化\n",
    "df1_exerpt[\"前のページ遷移\"][df1_exerpt[\"前のページ遷移\"].str.contains(www.olympus-medical.jp/gastroenterology/clinic, regex=True)] = 'gastroenterology/Clinic'\n",
    "df1_exerpt[\"ページ\"][df1_exerpt[\"ページ\"].str.contains(www.olympus-medical.jp/gastroenterology/clinic, regex=True)] = 'gastroenterology/Clinic'\n",
    "\n",
    "\n",
    "#['前のページ遷移', 'ページ']の組み合わせで重複したものを足し合わせ\n",
    "df1_exerpt = df1_exerpt[['前のページ遷移', 'ページ', 'ユーザー']].groupby(['前のページ遷移', 'ページ'], as_index=False).sum()\n",
    "\n",
    "df1_1 = df1_exerpt['前のページ遷移']\n",
    "df1_2 = df1_exerpt['ページ']\n",
    "df1_3 = df1_exerpt['ユーザー']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d300b2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# 参照元/メディア -> ページ\n",
    "\n",
    "# df2 = pd.read_csv('C:/Users/fumi9/Desktop/File/ReferenceMedia_PageView.csv')\n",
    "df2_exerpt = df2[df2['ユーザー']>PageViewFilter]\n",
    "\n",
    "# Lamda使って?以下を削除（?formId=があるものは残す）\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.split('?')[0] if (x.find('?') and not '?formId=' in x) else x)\n",
    "df2_exerpt[\"ページ\"] = pd.DataFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: x.split('?')[0] if (x.find('?') and not '?formId=' in x) else x)\n",
    "\n",
    "# Lamda使って二つのドメインのみをスコープにする\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x or ' / ' in x) and not 'olympusmedicalstg.prod.acquia-sites.com' in x else np.nan)\n",
    "df2_exerpt[\"ページ\"] = pd.DataFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x or ' / ' in x) else np.nan)\n",
    "\n",
    "\n",
    "# ()が悪さ？\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.replace(')', '').replace('(', '') if type(x) is str and ') / (' in x else x)\n",
    "\n",
    "\n",
    "# スペースが悪さ？\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.replace(' ', '_') if type(x) is str and ' ' in x else x)\n",
    "\n",
    "\n",
    "#Nanになったいらない行を削除\n",
    "df2_exerpt = df2_exerpt.dropna(how='any').reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "#個人所法登録フォーム一元化\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/MailForms\" if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' and not x.split('/')[1] == 'form' else x)\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/download-thanks\" if x.split('/')[1] == 'download-thanks' else x)\n",
    "df2_exerpt[\"参照元/メディア\"] = pd.DataFrame(df2_exerpt[\"参照元/メディア\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/Forms\" if x.split('/')[1] == 'form' else x)\n",
    "df2_exerpt[\"ページ\"] = pd.DataFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/MailForms\" if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' and not x.split('/')[1] == 'form' else x)\n",
    "df2_exerpt[\"ページ\"] = pd.DataFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/download-thanks\" if x.split('/')[1] == 'download-thanks' else x)\n",
    "df2_exerpt[\"ページ\"] = pd.DataFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/Forms\" if x.split('/')[1] == 'form' else x)\n",
    "\n",
    "# クリニック領域一元化\n",
    "df2_exerpt[\"ページ\"][df2_exerpt[\"ページ\"].str.contains(www.olympus-medical.jp/gastroenterology/clinic, regex=True)] = 'gastroenterology/Clinic'\n",
    "\n",
    "#['前のページ遷移', 'ページ']の組み合わせで重複したものを足し合わせ\n",
    "df2_exerpt = df2_exerpt[['参照元/メディア', 'ページ', 'ユーザー']].groupby(['参照元/メディア', 'ページ'], as_index=False).sum()\n",
    "\n",
    "df2_1 = df2_exerpt['参照元/メディア']\n",
    "df2_2 = df2_exerpt['ページ']\n",
    "df2_3 = df2_exerpt['ユーザー']\n",
    "\n",
    "df1_unique1 = df1_1.unique()\n",
    "df1_unique2 = df1_2.unique()\n",
    "df2_unique1 = df2_1.unique()\n",
    "df2_unique2 = df2_2.unique()\n",
    "df_unique = pd.Series(np.hstack((df1_unique1, df1_unique2, df2_unique1, df2_unique2))).unique()\n",
    "df_unique = pd.DataFrame(df_unique).applymap(lambda x: x.split('/')[-2] + \"/\" + x.split('/')[-1] if '/' in x else x)\n",
    "\n",
    "\n",
    "df_1= df2_1.append(df1_1).reset_index().drop('index', axis=1).to_numpy()\n",
    "df_2= df2_2.append(df1_2).reset_index().drop('index', axis=1).to_numpy()\n",
    "df_3= df2_3.append(df1_3).reset_index().drop('index', axis=1).to_numpy()\n",
    "\n",
    "\n",
    "df_1 = pd.DataFrame(df_1).applymap(lambda x: x.split('/')[-2] + \"/\" + x.split('/')[-1] if '/' in x else x)\n",
    "df_2 = pd.DataFrame(df_2).applymap(lambda x: x.split('/')[-2] + \"/\" + x.split('/')[-1] if '/' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e2279a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Position決めのためのWebページ種類分け \n",
    "groups = pd.DataFrame() \n",
    "\n",
    "# Main Groups\n",
    "groups['media'] = df_unique[df_unique.applymap(lambda x: True if '_/_' in x else False)]\n",
    "groups['top'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'www.olympus-medical.jp' else False)]\n",
    "groups['category'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'gastroenterology' else False)]\n",
    "groups['system'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'system' else False)]\n",
    "groups['scope'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'scope' else False)]\n",
    "groups['ai'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'ai' else False)]\n",
    "groups['et'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'et' else False)]\n",
    "groups['clinic'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'clinic' else False)]\n",
    "groups['onlineseminar'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'onlineseminar' else False)]\n",
    "groups['ma'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' else False)]\n",
    "groups['maTh'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[1] == 'download-thanks' else False)]\n",
    "groups['form'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'form' else False)]\n",
    " \n",
    "# Others\n",
    "groups['related'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'related' else False)]\n",
    "groups['preview'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'preview' else False)]\n",
    "groups['user'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'user' else False)]\n",
    "groups['node'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'node' else False)]\n",
    "groups['moderation'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'moderation' else False)]\n",
    "groups['movie'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'movie' else False)]\n",
    "groups['report'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'report' else False)]\n",
    "groups['information'] = df_unique[df_unique.applymap(lambda x: True if x.split('/')[0] == 'information' else False)]\n",
    "\n",
    "All_len = 0\n",
    "for column_name, group in groups.iteritems():\n",
    "    All_len = All_len + group.count()\n",
    "\n",
    "print(len(df_unique), All_len, \"equels?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bd979",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# df_unique.to_csv('D:/Users/fobayashi/Desktop/File/2022April/xxx.csv')\n",
    "# <span class=\"girk\">Networkxによる描画部</span>\n",
    "\n",
    "#円形配置：https://analytics-note.xyz/graph-theory/networkx-draw-pos/\n",
    "\n",
    "poss = []\n",
    "\n",
    "column_names = ['media',    'top', 'category', 'system', 'scope',  'ai',    'et',     'ma',   'clinic', 'onlineseminar', 'maTh', 'form', 'related',  'preview',  'user',    'node',  'moderation',  'movie',  'report', 'information']\n",
    "offsets      = [[-20, 0],   [0, 1], [0, 0],   [0, 20],  [0, 10], [-10, 20], [0, -30], [20, 0], [0, -20],    [14, 0],   [25, 0],   [30,0], [20, 10],   [20, 7],   [20, 4],  [25, -10],   [20, -2],     [20, -5],   [20, -8],   [20, -10]]\n",
    "Amps         = [[  7, 10],   [1, 1], [3, 4],   [3, 3],   [3, 3],   [1, 1],  [5, 5],   [3, 10], [3, 3],    [1, 1],       [1, 10], [1,10], [1, 1],    [1, 1],     [1, 1],   [1, 1],    [1, 1],      [1, 1],    [1, 1],    [1, 1]]\n",
    "\n",
    "for i in range(0, len(column_names)):\n",
    "    column_name = column_names[i]\n",
    "    offset = offsets[i]\n",
    "    Amp = Amps[i]\n",
    "    pos = {\n",
    "            n:  np.array([Amp[0]*np.cos(2*i*np.pi/groups[column_name].count())+offset[0], Amp[1]*np.sin(2*i*np.pi/groups[column_name].count())+offset[1]])\n",
    "            for i, n in enumerate(\"['\" + groups[column_name].dropna() + \"']\")\n",
    "        }\n",
    "    poss.append(pos)\n",
    "PosFin = {}\n",
    "\n",
    "for i in range(0, len(poss)):\n",
    "    PosFin.update(poss[i])\n",
    "# PosFin\n",
    "# 全体UUよりノードの大きさ決め"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f3804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# df3_exerpt.to_csv(\"D:/Users/fobayashi/Desktop/File/2022April/1.csv\")\n",
    "# # pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: x.split('?')[0] if (x.find('?')) else x)\n",
    "# df3_exerpt[\"ページ\"]\n",
    "# pd.DataFrame(df3_exerpt[\"ユーザー\"]).applymap(lambda x: x if x==\"\" else x)\n",
    "# df3_exerpt = df3\n",
    "# pd.DataFrame(df3_exerpt[\"ユーザー\"]).applymap(lambda x: x if type(x) is not float else 1)\n",
    "# df3_exerpt[type(df3_exerpt['ユーザー'])==int]\n",
    "# df3_exerpt['ページ']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5eb43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df3 = pd.read_csv('C:/Users/fumi9/Desktop/File/Page_AllUU.csv')\n",
    "df3_exerpt = df3\n",
    "df3_exerpt = df3_exerpt[df3_exerpt['ユーザー']>PageViewFilter]\n",
    "\n",
    "#Lamda使って?以下を削除（?formId=があるものは残す）\n",
    "# df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: x if type(x) is str else np.nan)\n",
    "df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: x.split('?')[0] if (type(x) is str and x.find('?') and not '?formId=' in x) else x)\n",
    "# df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: x.split('・・')[0] if (type(x) is str and x.find('・・')) else x)\n",
    "# df3_exerpt[\"ユーザー\"] = pd.DataFrame(df3_exerpt[\"ユーザー\"]).applymap(lambda x: x if type(x) is not float else 1)\n",
    "\n",
    "\n",
    "# Lamda使って二つのドメインのみをスコープにする\n",
    "df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: x if (type(x) is str and 'www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x) else np.nan)\n",
    " \n",
    "\n",
    "#Nanになったいらない行を削除\n",
    "df3_exerpt = df3_exerpt.dropna(how='any').reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "# 個人所法登録フォーム一元化\n",
    "df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/MailForms\" if x.split('/')[0] == 'oj-iryo2.olympus-medical.jp' and not x.split('/')[1] == 'download-thanks' and not x.split('/')[1] == 'form' else x)\n",
    "df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/download-thanks\" if x.split('/')[1] == 'download-thanks' else x)\n",
    "df3_exerpt[\"ページ\"] = pd.DataFrame(df3_exerpt[\"ページ\"]).applymap(lambda x: \"oj-iryo2.olympus-medical.jp/Forms\" if x.split('/')[1] == 'form' else x)\n",
    "\n",
    "# クリニック領域一元化\n",
    "df3_exerpt[\"ページ\"][df3_exerpt[\"ページ\"].str.contains(www.olympus-medical.jp/gastroenterology/clinic, regex=True)] = 'gastroenterology/Clinic'\n",
    "\n",
    "#['前のページ遷移', 'ページ']の組み合わせで重複したものを足し合わせ\n",
    "df3_exerpt = df3_exerpt[['ページ', 'ユーザー']].groupby(['ページ'], as_index=False).sum()\n",
    "\n",
    "df3_1 = df3_exerpt['ページ']\n",
    "df3_2 = df3_exerpt['ユーザー']\n",
    "df3_1_1 = pd.DataFrame(df3_1).applymap(lambda x: x.split('/')[-2] + \"/\" + x.split('/')[-1] if '/' in x else x)['ページ']\n",
    "\n",
    "#並べ替え\n",
    "Newdf3 = pd.DataFrame([df3_1_1,df3_2]).T\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df4 = pd.read_csv('C:/Users/fumi9/Desktop/File/ReferenceMedia_AllUU.csv')\n",
    "df4_exerpt = df4\n",
    "\n",
    "\n",
    "# # Lamda使って?以下を削除（?formId=があるものは残す）\n",
    "# df4_exerpt[\"参照元/メディア\"] = pd.DataFrame(df4_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.split('?')[0] if (type(x) is str and x.find('?') and not '?formId=' in x) else x)\n",
    "# df4_exerpt[\"ページ\"] = pd.DataFrame(df4_exerpt[\"ページ\"]).applymap(lambda x: x.split('?')[0] if (x.find('?') and not '?formId=' in x) else x)\n",
    "# # Lamda使って二つのドメインのみをスコープにする\n",
    "# df4_exerpt[\"参照元/メディア\"] = pd.DataFrame(df4_exerpt[\"参照元/メディア\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x or ' / ' in x) and not 'olympusmedicalstg.prod.acquia-sites.com' in x else np.nan)\n",
    "# df4_exerptdf4_exerptdf4_exerpt[\"ページ\"] = pd.Datdf4_exerptaFrame(df2_exerpt[\"ページ\"]).applymap(lambda x: x if type(x) is str and ('www.olympus-medical.jp' in x or 'oj-iryo2.olympus-medical.jp' in x or ' / ' in x) else np.nan)\n",
    "\n",
    "# ()が悪さ？\n",
    "df4_exerpt[\"参照元/メディア\"] = pd.DataFrame(df4_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.replace(')', '').replace('(', '') if type(x) is str and ') / (' in x else x)\n",
    "\n",
    "# スペースが悪さ？\n",
    "df4_exerpt[\"参照元/メディア\"] = pd.DataFrame(df4_exerpt[\"参照元/メディア\"]).applymap(lambda x: x.replace(' ', '_') if type(x) is str and ' ' in x else x)\n",
    "\n",
    "#Nanになったいらない行を削除\n",
    "df4_exerpt = df4_exerpt.dropna(how='any').reset_index().drop('index', axis=1)\n",
    "\n",
    "#['前のページ遷移', 'ページ']の組み合わせで重複したものを足し合わせ\n",
    "df4_exerpt = df4_exerpt[['参照元/メディア', 'ユーザー']].groupby(['参照元/メディア'], as_index=False).sum()\n",
    "df4_1 = df4_exerpt['参照元/メディア']\n",
    "df4_2 = df4_exerpt['ユーザー']\n",
    "df4_ = pd.DataFrame([df4_1, df4_2]).T.set_axis(['ページ', 'ユーザー'], axis=1)\n",
    "\n",
    "# 参照元/メディアとページを結合\n",
    "Newdf34 = Newdf3.append(df4_).reset_index(drop=True)\n",
    "df_unique_dummy = df_unique.copy()\n",
    "df_unique_copy = df_unique.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc317ec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T01:02:30.417072Z",
     "start_time": "2022-08-01T01:02:18.252611Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# networkxでグラフ\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# ↑jupter notebookで使用する場合のマジックコマンド\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "G = nx.DiGraph()\n",
    "for node in df_unique:\n",
    "    G.add_node(str(node))\n",
    "G.remove_node('0') #なぜか表示されていたので削除\n",
    "LineWeight = [n for n in df_3]\n",
    "\n",
    "for i in range(0, len(df_1)):\n",
    "#     print(str(df_1.to_numpy().tolist()[i]), str(df_2.to_numpy().tolist()[i]), LineWeight[i][0])\n",
    "    G.add_edge(str(df_1.to_numpy().tolist()[i]), str(df_2.to_numpy().tolist()[i]), weight=LineWeight[i][0]) #label='1')\n",
    "# dict(G.degree()) \n",
    "\n",
    "tmp = pd.DataFrame(G.nodes()).applymap(lambda x: x.replace('[', '').replace(']', '').replace(\"'\", \"\") if type(x) is str and '[' in x else x)\n",
    "\n",
    "# 遷移数に存在するリンクの全体UUをnodesの並びと同じにする\n",
    "df_unique_copy = tmp\n",
    "\n",
    "for i in range(0, len(df_unique_copy)):\n",
    "    df_unique_dummy.iloc[i,0] = Newdf34[Newdf34['ページ'] == df_unique_copy.iloc[i,0]]['ユーザー'].sum()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "# pos = nx.spring_layout(G, k=100, iterations=3, seed=3)\n",
    "# pos = nx.circular_layout(G)\n",
    "pos = PosFin\n",
    "\n",
    "# for node in G.nodes():\n",
    "#     G.nodes[node][\"pos\"] = pos[node]\n",
    "\n",
    "# widthをGの並び替え\n",
    "# dfNew: ユーザー数アリ、dfG: G.edgesの並び\n",
    "dfNew = pd.concat([df_1[0], df_2[0], pd.DataFrame(df_3)[0]], axis=1).set_axis(['前ページ', 'ページ', 'ユーザー'], axis=1)\n",
    "dfG = pd.DataFrame(columns=['前ページ', 'ページ'])\n",
    "\n",
    "i=0\n",
    "for edge in G.edges():\n",
    "    dfG = dfG.append(pd.DataFrame({(edge[0], edge[1])}, columns=['前ページ', 'ページ']),ignore_index=True)\n",
    "    i=i+1\n",
    "dfG = dfG.applymap(lambda x: x.replace('[', '').replace(']', '').replace(\"'\", \"\") if type(x) is str and '[' in x else x)\n",
    "\n",
    "dfNew['前後ページ'] = pd.DataFrame(dfNew['前ページ'] + '&' + dfNew['ページ'])\n",
    "dfG['前後ページ'] = pd.DataFrame(dfG['前ページ'] + '&' + dfG['ページ'])\n",
    "\n",
    "\n",
    "dfNew = dfNew.drop('前ページ', axis=1).drop('ページ', axis=1)\n",
    "dfG = dfG.drop('前ページ', axis=1).drop('ページ', axis=1)\n",
    "dfLineWidth_dummy = dfNew.copy()\n",
    "\n",
    "for i in range(0, len(df_unique_copy)):\n",
    "    dfLineWidth_dummy.iloc[i,0] = dfNew[dfNew['前後ページ'] == dfG.iloc[i,0]]['ユーザー']#.sum()\n",
    "\n",
    "\n",
    "# LineSize networkxで表示するとなんか変、Gephiはあってる\n",
    "LineSize = [np.array([n/200]) for n in dfLineWidth_dummy['ユーザー']]\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.6, edge_color='g',  width=LineSize)\n",
    "\n",
    "df34ForColor = df_unique_copy.copy()\n",
    "df34ForColor.columns=['ページ']\n",
    "df34ForColor['color'] = 'blue'\n",
    "for i in range(0, len(df34ForColor['ページ'])):\n",
    "    if '_/_' in df34ForColor['ページ'][i]:\n",
    "        df34ForColor['color'][i] = 'yellow'\n",
    "    elif 'oj-iryo2' in df34ForColor['ページ'][i]:\n",
    "        df34ForColor['color'][i] = 'black'\n",
    "    else:\n",
    "        df34ForColor['color'][i] = 'blue'\n",
    "# nx.draw_networkx_nodes(G, pos, node_color='b', alpha=0.5)\n",
    "nx.draw_networkx_nodes(G, pos, node_color=df34ForColor['color'], alpha=0.5, node_size=df_unique_dummy.to_numpy().tolist()) #size指定\n",
    "\n",
    "\n",
    "# ラベルつけるかどうか\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, alpha=1)\n",
    "plt.show()\n",
    "fig.savefig(\"img.png\")\n",
    "\n",
    "# 出力\n",
    "sizes = df_unique_dummy.to_numpy().tolist()\n",
    "poss = PosFin\n",
    "\n",
    "df34ForColor['rgb']=df34ForColor['color'].copy()\n",
    "for i in range(0, len(df34ForColor['rgb'])):\n",
    "    if df34ForColor['color'][i] == 'blue':\n",
    "        df34ForColor['rgb'][i]={'r': 0, 'g': 0, 'b': 255, 'a': 1}\n",
    "    elif df34ForColor['color'][i] == 'yellow':\n",
    "        df34ForColor['rgb'][i]={'r': 255, 'g': 255, 'b': 0, 'a': 1}\n",
    "    elif df34ForColor['color'][i] == 'black':\n",
    "        df34ForColor['rgb'][i]={'r': 0, 'g': 0, 'b': 0, 'a': 1}\n",
    "    else:\n",
    "        df34ForColor['rgb'][i]={'r': 128, 'g': 0, 'b': 128, 'a': 1}\n",
    "\n",
    "i=0\n",
    "for g in G.nodes():\n",
    "    G.nodes[g]['viz'] = {'size': sizes[i][0],\n",
    "#                         'color': {'r': color[g][0], 'g': color[g][1], 'b': color[g][2], 'a': 1},\n",
    "#                         'color': {'r': 0, 'g': 0, 'b': 255, 'a': 0.5},\n",
    "                         'color': df34ForColor['rgb'][i],\n",
    "                        'position': {'x': poss[g][0]*1000, 'y': poss[g][1]*1000, 'z': 0}}\n",
    "    i=i+1\n",
    "\n",
    "for g in G.edges():\n",
    "    G.edges[g]['viz'] = {'color': { 'r': 0, 'b': 0, 'g': 255, 'a': 0.6}}\n",
    "\n",
    "#あとノード色変えたい\n",
    "nx.write_gexf(G, \"./test2.gexf\", version = '1.2draft') \n",
    "\n",
    "# 出力したgexfファイルのLabelにAllUUの数値を追加\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('./test2.gexf')\n",
    "# tree = ET.parse('./sports.xml')\n",
    "\n",
    "# XMLを取得\n",
    "root = tree.getroot()\n",
    "\n",
    "# AllUUのこと\n",
    "sizesForXml = df_unique_dummy.to_numpy().tolist()\n",
    "\n",
    "i=0\n",
    "for child in root[1][0]: # nodes\n",
    "    child.attrib['label'] = child.attrib['label'] + \":\" + str(sizesForXml[i][0])\n",
    "    i=i+1\n",
    "\n",
    "# XMLファイル書き込み\n",
    "tree.write('./test3_draw.gexf', encoding='UTF-8')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc3d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
